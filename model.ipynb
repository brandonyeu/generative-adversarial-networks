{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1BB1+djFvWxmlb/xUSJum"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Networks\n",
        "### Brandon Yeu\n",
        "\n",
        "1. GAN training is modeled as a two-player minimax game between a generator, which tries to produce fake data that appears real, and a discriminator, which tries to identify real or fake data. The generator's goal is to minimize the discriminator's success, but the discriminator's goal is to maximize its success.\n",
        "\n",
        "2. GANs are powerful, but it can often be difficult to train them. One common challenge is mode collapse, in which the generator produces a limited variety of outputs, as opposed to a diverse set of realistic data. This can cause the model to map different latent inputs to the same/similar samples. Mode collapse can occur if the reward function is not diverse or if the generator finds a quick way to exploit the discriminator. Techniques such as batch normalization, minibatch discrimination, and Wassertein GAN can help mitigate mode collapse.\n",
        "\n",
        "3. In an adversarial network, the discriminator is an adaptive loss function for the generator. It helps to dynamically train the model, and it improves with the generator continuously redefining how they function during training.\n",
        "\n",
        "4. Inception score (IS) evaluates GAN performances by rewarding high confidence predictions and high diversity, but it ignores real data distribution, can be bypassed by sharp, but unrealistic images, and is sensitive to the pretrained classifier. Fréchet inception distance (FID) evaluates GAN performances by comparing statistics of real and generated images in feature space. FID measures image quality compared to real images and diversity, so it better correlates with human comparison and penalizes mode collapse."
      ],
      "metadata": {
        "id": "IeX89JQ2u2AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load and preprocess data\n",
        "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize to [-1, 1]\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Generator model\n",
        "def make_generator_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Reshape((7, 7, 256)),\n",
        "      layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "# Discriminator model\n",
        "def make_discriminator_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "# Loss functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  return real_loss + fake_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-ngZaNQu8tA",
        "outputId": "b5e8e883-b995-4705-8f89-43053d0637c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Optimizers\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Training function\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  noise = tf.random.normal([BATCH_SIZE, 100])\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(noise, training=True)\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "# Training loop\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "# Run the training\n",
        "train(train_dataset, epochs=1)"
      ],
      "metadata": {
        "id": "c5vvm3ntu_qr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fneNGhv4lx8L",
        "outputId": "db2d001a-5452-4450-b51d-7c53f4815f6c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Time: 50.32 sec\n",
            "Epoch 2, Time: 38.95 sec\n",
            "Epoch 3, Time: 39.20 sec\n",
            "Epoch 4, Time: 38.85 sec\n",
            "Epoch 5, Time: 39.34 sec\n",
            "Epoch 6, Time: 38.97 sec\n",
            "Epoch 7, Time: 38.98 sec\n",
            "Epoch 8, Time: 39.04 sec\n",
            "Epoch 9, Time: 39.08 sec\n",
            "Epoch 10, Time: 39.64 sec\n",
            "Epoch 11, Time: 39.13 sec\n",
            "Epoch 12, Time: 39.15 sec\n",
            "Epoch 13, Time: 39.20 sec\n",
            "Epoch 14, Time: 39.02 sec\n",
            "Epoch 15, Time: 39.21 sec\n",
            "Epoch 16, Time: 39.09 sec\n",
            "Epoch 17, Time: 38.92 sec\n",
            "Epoch 18, Time: 38.93 sec\n",
            "Epoch 19, Time: 38.96 sec\n",
            "Epoch 20, Time: 39.20 sec\n",
            "Epoch 21, Time: 38.98 sec\n",
            "Epoch 22, Time: 39.06 sec\n",
            "Epoch 23, Time: 39.08 sec\n",
            "Epoch 24, Time: 39.16 sec\n",
            "Epoch 25, Time: 39.03 sec\n",
            "Epoch 26, Time: 38.97 sec\n",
            "Epoch 27, Time: 38.98 sec\n",
            "Epoch 28, Time: 39.01 sec\n",
            "Epoch 29, Time: 39.01 sec\n",
            "Epoch 30, Time: 39.21 sec\n",
            "Epoch 31, Time: 39.05 sec\n",
            "Epoch 32, Time: 39.15 sec\n",
            "Epoch 33, Time: 38.93 sec\n",
            "Epoch 34, Time: 39.23 sec\n",
            "Epoch 35, Time: 39.01 sec\n",
            "Epoch 36, Time: 39.20 sec\n",
            "Epoch 37, Time: 39.01 sec\n",
            "Epoch 38, Time: 40.94 sec\n",
            "Epoch 39, Time: 39.32 sec\n",
            "Epoch 40, Time: 39.28 sec\n",
            "Epoch 41, Time: 38.93 sec\n",
            "Epoch 42, Time: 39.02 sec\n",
            "Epoch 43, Time: 39.06 sec\n",
            "Epoch 44, Time: 39.10 sec\n",
            "Epoch 45, Time: 39.10 sec\n",
            "Epoch 46, Time: 39.14 sec\n",
            "Epoch 47, Time: 39.13 sec\n",
            "Epoch 48, Time: 39.12 sec\n",
            "Epoch 49, Time: 39.15 sec\n",
            "Epoch 50, Time: 39.71 sec\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 50\n",
        "NOISE_DIM = 100\n",
        "NUM_EXAMPLES_TO_GENERATE = 16\n",
        "\n",
        "# Load and preprocess data (CIFAR-10)\n",
        "(train_images, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_images = train_images.astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\\\n",
        "    .shuffle(BUFFER_SIZE)\\\n",
        "    .batch(BATCH_SIZE)\n",
        "\n",
        "# Generator model\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(4 * 4 * 512, use_bias=False, input_shape=(NOISE_DIM,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Reshape((4, 4, 512)),\n",
        "\n",
        "        layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same',\n",
        "                               use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                      input_shape=[32, 32, 3]),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# Loss functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "\n",
        "# Training function\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([tf.shape(images)[0], NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(\n",
        "        gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(\n",
        "        disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "# Image saving\n",
        "seed = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])\n",
        "os.makedirs(\"generated_images\", exist_ok=True)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "    predictions = (predictions + 1) / 2.0\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig(f'generated_images/image_at_epoch_{epoch:04d}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Training loop\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Time: {time.time() - start:.2f} sec')\n",
        "\n",
        "    generate_and_save_images(generator, epochs, seed)\n",
        "\n",
        "# Run training\n",
        "train(train_dataset, EPOCHS)"
      ]
    }
  ]
}